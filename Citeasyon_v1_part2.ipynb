{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "Mn3DNWByYJ1T"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "NUI6F9fv6VJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT!! You must be connected to a GPU or TPU in order to be able to load the Huggingface model."
      ],
      "metadata": {
        "id": "Z3V2gbmv6jIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dependencies"
      ],
      "metadata": {
        "id": "M6NwSDKI6zLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#module2\n",
        "\n",
        "!pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "id": "_IMG8vMGI5qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 2\n",
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "V1HTE6TYI8V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "lQKAoANWI9sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -w -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMiUcyejn_5U",
        "outputId": "70acafb7-28db-4910-f78c-0c856c25f79d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget: --wait: Invalid time period ‘-O’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Modules"
      ],
      "metadata": {
        "id": "JRNBU-ld8GNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile load_hf.py\n",
        "from huggingface_hub import login # notebook_login\n",
        "\n",
        "def load_huggingface_accesstoken(token):\n",
        "  \"\"\"\n",
        "  This function loads huggingface access token so you can get access to models such as Llama2\n",
        "  \"\"\"\n",
        "  try:\n",
        "    login(token=token)\n",
        "    print(\"Successfully logged in to Hugging Face\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to log in to Hugging Face: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyztPpIIx9v5",
        "outputId": "0a4f3e32-b9af-4015-96d3-b4123247018d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing load_hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module1.py\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "def read_csv(csv_file):\n",
        "    \"\"\"\n",
        "    This function reads a .csv file, confirms it is a valid Citeasyon format, and returns a dataframe.\n",
        "    Any column not found in the expected columns will be removed.\n",
        "    \"\"\"\n",
        "\n",
        "    expected_citeasyon_cols = [\"file_id\", \"title\", \"author_name\", \"abstract\", \"keywords\",\n",
        "                               \"journal\", \"volume\", \"proceedings\", \"year\", \"month\", \"doi\",\n",
        "                               \"url\", \"pages\", \"field\", \"topic\"]\n",
        "\n",
        "    try:\n",
        "        # read .csv, automatically detect and remove any index column like 'Unnamed: 0'\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # normalize column names in both lists\n",
        "        df.columns = [col.strip().lower() for col in df.columns]\n",
        "        expected_columns = [col.strip().lower() for col in expected_citeasyon_cols]\n",
        "\n",
        "        # keep only the columns that are in the expected columns list\n",
        "        df = df[[col for col in df.columns if col in expected_columns]]\n",
        "\n",
        "        # if the remaining columns match the expected columns\n",
        "        if set(df.columns) == set(expected_columns):\n",
        "            st.success(\"CSV file is valid Citeasyon FORMAT\")\n",
        "            return df\n",
        "        else:\n",
        "            st.error(f\"CSV file is not valid Citeasyon FORMAT. Expected columns: {expected_citeasyon_cols}\")\n",
        "            st.error(f\"Found columns: {df.columns.tolist()}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# upload and process the .csv file\n",
        "def load_data():\n",
        "    uploaded_file = st.file_uploader(\"Select your Citeasyon DataBase\", type=[\"csv\"], accept_multiple_files=False)\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            input_database = read_csv(uploaded_file)\n",
        "            if input_database is not None:\n",
        "                st.write(input_database)\n",
        "                return input_database\n",
        "            else:\n",
        "                st.warning(\"No valid data to process.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing file: {e}\")\n",
        "            return None\n",
        "    else:\n",
        "        st.info(\"Please upload a CSV file.\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "v8taVv2mpRHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c87e93-3214-49ab-e2e1-87a0af825d96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing module1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41GbeThRV--G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 2 version 3\n",
        "\n",
        "%%writefile module2.py\n",
        "\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import torch\n",
        "\n",
        "def load_model():\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"  # model in bin format\n",
        "    return model_name_or_path, model_basename\n",
        "\n",
        "def model_gpu(model_path):\n",
        "    # Initialize model with optimized parameters for memory management\n",
        "    lcpp_llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_threads=2,  # Use 2 CPU cores\n",
        "        n_batch=128,  # reduced batch size for lower memory usage\n",
        "        n_gpu_layers=20,  # adjusted GPU layers\n",
        "        n_ctx=2048,  # reduced context window to fit within memory\n",
        "    )\n",
        "    return lcpp_llm\n",
        "\n",
        "\n",
        "\n",
        "def llm_citation_assistant(citation_type, gpu_for_model, database):\n",
        "    dictionary_citations = {}\n",
        "\n",
        "    # iterate over each row in the database\n",
        "    for index, row in database.iterrows():\n",
        "        # construct the prompt from row information\n",
        "        information_article = f\"\"\"[s]Title: {row[\"title\"]}[/s] [s]Authors: {row[\"author_name\"]}[/s] [s]Journal: {row[\"journal\"]}[/s] [s]Proceedings: {row[\"proceedings\"]}[/s] [s]Volume: {row[\"volume\"]}[/s] [s]Year: {row[\"year\"]}[/s] [s]DOI: {row[\"doi\"]}[/s] [s]Pages: {row[\"pages\"]}[/s]\"\"\"\n",
        "        full_prompt = information_article\n",
        "\n",
        "        prompt_template = f'''\n",
        "SYSTEM: You are in charge of formatting citations into the latest version known of {citation_type} style. Your output is the final citations version. If there is an element missing, you put an informative placeholder like TITLE if title is missing. Return the citation directly, no explanation.\n",
        "USER: ARTICLE INFORMATION: {full_prompt}\n",
        "ASSISTANT:'''\n",
        "\n",
        "        try:\n",
        "            response = gpu_for_model(\n",
        "                prompt=prompt_template,\n",
        "                max_tokens=256,\n",
        "                temperature=0.1,\n",
        "                top_p=0.95,\n",
        "                repeat_penalty=1.2,\n",
        "                top_k=50,\n",
        "                stop=['USER:', 'SYSTEM:', 'ASSISTANT:'],\n",
        "                echo=False  # Do not echo the prompt back\n",
        "            )\n",
        "\n",
        "            # extract response from the models answer\n",
        "            assistant_response = response[\"choices\"][0][\"text\"].strip()\n",
        "\n",
        "            # make sure the response is cleaned up and only contains the citation\n",
        "            assistant_response = assistant_response.split('ASSISTANT:')[-1].strip()\n",
        "\n",
        "            # store results in dictionary format\n",
        "            dictionary_citations[row[\"file_id\"]] = assistant_response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index} ({row['file_id']}): {e}\")\n",
        "            dictionary_citations[row[\"file_id\"]] = \"Error\"\n",
        "\n",
        "    return dictionary_citations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def citation_generator(citation_type, database):\n",
        "    model_name_or_path, model_basename = load_model()\n",
        "\n",
        "    # Download model and log path\n",
        "    try:\n",
        "        model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "        print(f\"Model downloaded to: {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download model: {e}\")\n",
        "        return\n",
        "\n",
        "    # Load model to GPU\n",
        "    try:\n",
        "        gpu_for_model = model_gpu(model_path=model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model to GPU: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"Module 2: citation_generator started\")\n",
        "\n",
        "    dictionary_citations = {}\n",
        "\n",
        "    # Extract information and update citations\n",
        "    if citation_type:\n",
        "        dictionary_citations = llm_citation_assistant(citation_type, gpu_for_model, database)\n",
        "        print(dictionary_citations)\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid citation type or not implemented yet\")\n",
        "\n",
        "    return dictionary_citations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG1k-Vt9BPeq",
        "outputId": "3915b0f8-ee77-4fb7-b8b8-f16027915b31"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting module2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8SFHkiyBPbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. app.py"
      ],
      "metadata": {
        "id": "9z7k0nnI8qmE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yjYKCdcD0Zfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# version 3\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "from load_hf import load_huggingface_accesstoken\n",
        "from module1 import load_data\n",
        "from module2 import citation_generator\n",
        "\n",
        "st.set_page_config(page_title='Citeasyon', page_icon=\":tada:\", layout=\"wide\")\n",
        "\n",
        "# HEADER\n",
        "with st.container():\n",
        "    st.title(\":red[_CITEASYON step 2_] :sparkles:\")\n",
        "\n",
        "\n",
        "def run_citeasyon_step2():\n",
        "    ## 0. hugging face access token\n",
        "    token = st.text_input(\"Enter your Hugging Face access Token\", type=\"password\")\n",
        "    if token:\n",
        "        load_huggingface_accesstoken(token)\n",
        "    else:\n",
        "        st.warning(\"Access token for Hugging Face required\")\n",
        "        return  # Exit if token is not provided\n",
        "\n",
        "    ## 1. load data\n",
        "    input_database = load_data()\n",
        "\n",
        "    # choose MLA or APA citation style\n",
        "    st.sidebar.header(\"Citation Format\")\n",
        "    citation_options = st.sidebar.multiselect(\"Select:\", [\"MLA\", \"APA\"])\n",
        "\n",
        "    # check if the user has selected at least one citation style\n",
        "    if not citation_options:\n",
        "        st.warning(\"Please select at least one citation format.\")\n",
        "        return  # exit if no citation format is selected\n",
        "\n",
        "    # \"Go\" button to start the citation\n",
        "    if st.sidebar.button(\"Go\"):\n",
        "        # init citations dictionary in session state\n",
        "        if 'citations_dictionary' not in st.session_state:\n",
        "            st.session_state.citations_dictionary = {}\n",
        "\n",
        "        for citation_type in citation_options:\n",
        "            cites_complete = citation_generator(citation_type, input_database)\n",
        "\n",
        "            # update session state dictionary with new citations\n",
        "            st.session_state.citations_dictionary.update(cites_complete)\n",
        "\n",
        "            st.write(f\"Generated Citations for {citation_type}:\")\n",
        "            for cite in cites_complete.values():\n",
        "                st.write(cite)\n",
        "\n",
        "            citations_text = \"\\n\".join(cites_complete.values())\n",
        "            st.download_button(\n",
        "                label='Download citations',\n",
        "                data=citations_text,\n",
        "                file_name=f'{citation_type}_citeasyons.txt',\n",
        "                mime='text/plain'\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        st.info(\"Click 'Go' to start the citation generation process.\")\n",
        "\n",
        "run_citeasyon_step2()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxF4KAWm9ATc",
        "outputId": "8fd457db-73b2-4cdb-c4fe-981f300d2610"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Run local tunnel"
      ],
      "metadata": {
        "id": "Eh72jbvlH-M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "id": "LrRqv1OOJbLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRLJydoThZmm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}