{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "NUI6F9fv6VJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTANT!! You must be connected to a GPU or TPU in order to be able to load the Huggingface model."
      ],
      "metadata": {
        "id": "Z3V2gbmv6jIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dependencies & pips"
      ],
      "metadata": {
        "id": "-pEyzM-ALSPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#module4 part 1\n",
        "\n",
        "!pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "id": "1tVpLLzYwIZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 4 part 2\n",
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "id": "_g0487-SwGYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit"
      ],
      "metadata": {
        "id": "uUuhUNaGwCb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -w -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMiUcyejn_5U",
        "outputId": "21c03850-c36d-4ac9-c5db-7bac38581dcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wget: --wait: Invalid time period ‘-O’\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "IAnhfZsOv_hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "cEZAAFpVv9Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "id": "HHsJKVEcv7jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "id": "JDQCxBafv6NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 pandas==1.5.3"
      ],
      "metadata": {
        "id": "9GTjoasBv3cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMVYOA_6v30q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Continue run here after session restart"
      ],
      "metadata": {
        "id": "FkibNaaDFe26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# module1\n",
        "!pip install pymupdf #fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBUDrTdTn_z-",
        "outputId": "f94256bd-8166-498d-ee3b-86d657f8d15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#module2\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjHuhg24zFDk",
        "outputId": "d64ceec4-3ed7-4902-e4ab-c2856ca19607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EnSe3in_6ht3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modules"
      ],
      "metadata": {
        "id": "JRNBU-ld8GNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile load_hf.py\n",
        "from huggingface_hub import login # notebook_login\n",
        "\n",
        "def load_huggingface_accesstoken(token):\n",
        "  \"\"\"\n",
        "  function loads huggingface access token (so you can get access to models such as Llama2, llama3)\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    login(token=token)\n",
        "    print(\"Successfully logged in to Hugging Face\")\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to log in to Hugging Face: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyztPpIIx9v5",
        "outputId": "894008d7-1810-4db0-cc45-e295c439a247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing load_hf.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module1.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import pdf2image\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Option 1. Read .txt file\n",
        "def read_txt(txt_file):\n",
        "  try:\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "      readable_file = f.read()\n",
        "  except Exception as e: # try another encoding (latin-1)\n",
        "    st.error(f\"Error handling TXT file with UTF-8 encoding: {e}\")\n",
        "    try:\n",
        "      with open(txt_file, \"r\", encoding=\"latin-1\") as f:\n",
        "        readable_file= f.read()\n",
        "    except Exception as e:\n",
        "      st.error(f\"Error reading TXT file with Latin-1 encoding: {e}\")\n",
        "      return None\n",
        "\n",
        "  return readable_file\n",
        "\n",
        "\n",
        "# Option 2. read .pdf file\n",
        "def read_pdf(pdf_file):\n",
        "    try:\n",
        "        doc = fitz.open(stream= pdf_file.read(), filetype = \"pdf\") # pen .pdf file\n",
        "        text = \"\"\n",
        "        # iterate through each page of the .pdf\n",
        "        for page in doc:\n",
        "            text += page.get_text()  # extract text from the current page\n",
        "        doc.close() # close doc\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR while reading PDF with fitz: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ocr reader\n",
        "def ocr_for_pdf(file, n_max_tokens =1000):\n",
        "  st.info(\"OCR loading...\")\n",
        "  file.seek(0) # reset file pointer\n",
        "  images = pdf2image.convert_from_bytes(file.read())\n",
        "  complete_text = []\n",
        "  #counter_pages=0\n",
        "  for page_number, image in enumerate(images):\n",
        "    #counter_pages = counter_pages +1\n",
        "    #st.write(\"Page number:\", counter_pages)\n",
        "    #st.image(page, use_column_width = True) # display image\n",
        "    text= pytesseract.image_to_string(image) # read text using pytesseract\n",
        "\n",
        "\n",
        "    ## adustment to avoid loading and reading the whole paper: just get the first n tokens\n",
        "    #complete_text.append(text) # this is to get the whole text\n",
        "    tokens = text.split()\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "\n",
        "    if total_tokens + num_tokens < n_max_tokens:\n",
        "      complete_text.append(text)\n",
        "      total_tokens += num_tokens\n",
        "    else:\n",
        "      # calculate remaining_tokens\n",
        "      remaining_tokens = n_max_tokens -total_tokens\n",
        "      complete_text.append(\" \").join(tokens[:remaining_tokens])\n",
        "      break\n",
        "    #num_pages = len(images)\n",
        "\n",
        "  return \" \".join(complete_text)\n",
        "\n",
        "\n",
        "# Upload a file\n",
        "def load_data():\n",
        "    uploaded_data = st.file_uploader(\":grey[Select your files]\", type= [\"pdf\", \"txt\"], accept_multiple_files=True)\n",
        "    content_dic = {}\n",
        "    st.warning(\"Press the button once all the files are loaded\")\n",
        "    if st.button(\"Process Files\"): #uploaded_data is not None:\n",
        "      for file in uploaded_data:\n",
        "\n",
        "        if file.name.endswith(\".txt\"):\n",
        "            content = read_txt(txt_file = file)\n",
        "            st.write(content)\n",
        "            content_dic[file.name] = content\n",
        "\n",
        "        elif file.name.endswith(\".pdf\"):\n",
        "          try:\n",
        "            # Read and print the text from the PDF\n",
        "            content = read_pdf(pdf_file = file)\n",
        "\n",
        "            if content is None:\n",
        "                st.info(\"OCR is required. The process may take longer\")\n",
        "                final_text = ocr_for_pdf(file, n_max_tokens=1000)\n",
        "                content = \"\\n\".join(final_text) # output of ocr function is list, transform to string\n",
        "\n",
        "            if content is not None:\n",
        "                #st.write(content)\n",
        "                content_dic[file.name] = content\n",
        "            else:\n",
        "              st.error(f\"Failed to extract text from PDF: {file.name}\")\n",
        "\n",
        "          except Exception as e:\n",
        "            st.error(f\"An error occured while reading the pdf: {e}\")\n",
        "\n",
        "    #st.write(\"module 1\",content_dic.keys(), content_dic.values())\n",
        "    return content_dic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW_6OY2ln_i4",
        "outputId": "7bbd5d49-4c7e-4a9b-cbb2-290966a0a0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting module1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module2.py\n",
        "import streamlit as st\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "#####\n",
        "\n",
        "\n",
        "def shorten_content(content_dic: dict, n_max_tokens: int) -> dict:\n",
        "  small_content_dic = {}\n",
        "  for key, value in content_dic.items():\n",
        "    tokens = nltk.word_tokenize(value)\n",
        "    #num_tokens = len(tokens)\n",
        "    small_content_dic[key] =  \" \".join(tokens[:n_max_tokens]) # first n_max_tokens tokens\n",
        "\n",
        "  return small_content_dic\n",
        "\n",
        "\n",
        "def divide_content(content_dic: dict) -> dict:\n",
        "    \"\"\"Divide content into parts based on abstract and keyword sections.\"\"\"\n",
        "    content_dic_parts = {}\n",
        "\n",
        "    abstract_list = [\"abstract\"]\n",
        "    keywords_list = [\"keywords\"]\n",
        "\n",
        "    for key, value in content_dic.items():\n",
        "        tokens = nltk.word_tokenize(value)\n",
        "        abstract_index = next((i for i, token in enumerate(tokens) if any(w in token.lower() for w in abstract_list)), None)\n",
        "        keyword_index = next((i for i, token in enumerate(tokens) if any(w in token.lower() for w in keywords_list)), None)\n",
        "\n",
        "        # debugging output\n",
        "        print(f\"Processing file: {key}\")\n",
        "        print(f\"Total tokens: {len(tokens)}\")\n",
        "        print(f\"Tokens preview: {tokens[:50]}...\")  # print first 50 tokens\n",
        "        print(f\"Abstract index: {abstract_index}, Keyword index: {keyword_index}\")\n",
        "\n",
        "        # Initialize parts\n",
        "        part1 = part2 = part3 = \"\"\n",
        "\n",
        "        if abstract_index is not None and keyword_index is not None:\n",
        "            part1 = \" \".join(tokens[:abstract_index])\n",
        "            part2 = \" \".join(tokens[abstract_index:keyword_index])\n",
        "            part3 = \" \".join(tokens[keyword_index:])\n",
        "        elif abstract_index is not None:\n",
        "            part1 = \" \".join(tokens[:abstract_index])\n",
        "            part2 = \" \".join(tokens[abstract_index:])\n",
        "        elif keyword_index is not None:\n",
        "            part1 = \" \".join(tokens[:keyword_index])\n",
        "            part3 = \" \".join(tokens[keyword_index:])\n",
        "        else:\n",
        "            part1 = value  # all content goes directly to part1 if neither index is found\n",
        "\n",
        "        # avoid duplication and ensure correct assignment\n",
        "        if not part2 and not part3:\n",
        "            print(f\"Warning: No abstract or keywords found for file {key}\")\n",
        "\n",
        "        content_dic_parts[key] = [part1, part2, part3]\n",
        "\n",
        "        # debugging output\n",
        "        print(f\"Parts for file {key}: Part 1 length = {len(part1)}, Part 2 length = {len(part2)}, Part 3 length = {len(part3)}\")\n",
        "\n",
        "    return content_dic_parts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_data(content_dic):\n",
        "  if content_dic == None:\n",
        "    st.error(\"No files were uploaded\")\n",
        "    return None\n",
        "  else:\n",
        "    #st.write(\"Files were uploaded\")\n",
        "    smaller_content_dic = shorten_content(content_dic, n_max_tokens = 1000)\n",
        "    content_dic_parts = divide_content(content_dic = smaller_content_dic)\n",
        "  return content_dic_parts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTMm-OKTymUn",
        "outputId": "fb02bd88-67ac-4030-c226-b92a5f5101ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing module2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module3.py\n",
        "\n",
        "import pandas as pd\n",
        "def create_csv_file():\n",
        "  database = pd.DataFrame(columns=[\"file_id\", \"title\", \"author_name\", \"abstract\", \"keywords\",\n",
        "                          \"journal\", \"volume\", \"proceedings\",  \"year\", \"month\", \"doi\", \"url\", \"pages\", \"field\", \"topic\"]) # TODO: You can add as many columns as you want,but REMEMBER to add the prompt in the module 4\n",
        "  return database\n",
        "\n",
        "\n",
        "def fillin_database(database, content_dic):\n",
        "  database[\"file_id\"] = content_dic.keys()\n",
        "  database[\"abstract\"] = \"\"\n",
        "  for key, value in content_dic.items():\n",
        "    if len(value) == 3 and all(v is not None for v in value[:3]):\n",
        "      database.loc[database[\"file_id\"] == key, \"abstract\"] = value[1]\n",
        "    # add abstract directly to content_dic for use in the next module!!\n",
        "    content_dic[key].append(value[1])\n",
        "\n",
        "  return database, content_dic  # return updated content_dic\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUBPrdpE3NtS",
        "outputId": "a322bb46-84f1-4b3d-8c1d-7b2d717c43b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing module3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMClSYa7CH85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile module4.py\n",
        "\n",
        "import pandas as pd\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import torch\n",
        "\n",
        "def load_model():\n",
        "    model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "    model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "    return model_name_or_path, model_basename\n",
        "\n",
        "def model_gpu(model_path):\n",
        "    lcpp_llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_threads=2,\n",
        "        n_batch=128,\n",
        "        n_gpu_layers=20,\n",
        "        n_ctx=2048,\n",
        "    )\n",
        "    return lcpp_llm\n",
        "\n",
        "prompt_dic = {\n",
        "    \"abstract\": \"If one, extract the abstract of this text and return it. Otherwise return NONE\",\n",
        "    \"author_name\": \"If one, extract the author/s names of this text. Otherwise return NONE\",\n",
        "    \"doi\": \"If one, extract the DOI of this text. Otherwise return NONE\",\n",
        "    \"title\": \"If one, extract the title of this text. Otherwise return NONE\",\n",
        "    \"keywords\": \"If one, extract the keywords from the Keyword section of this text. Otherwise return NONE\",\n",
        "    \"pages\": \"If one, extract the pages where this article was published. Otherwise return NONE\",\n",
        "    \"year\": \"If one, extract ONLY the year of publication of this text. Otherwise return NONE\",\n",
        "    \"month\": \"If one, extract ONLY the month of publication of this text. Otherwise return NONE\",\n",
        "    \"url\": \"If one, extract ONLY the URL of publication of this text. Otherwise return NONE\",\n",
        "    \"journal\": \"If one, extract ONLY the journal name of publication of this text\",\n",
        "    \"volume\": \"If one, extract ONLY the Volume number or issue of publication of this text. Otherwise return NONE\",\n",
        "    \"proceedings\": \"If one, extract ONLY the Proceedings where this text was published. If it is other than Proceedings (example:journal), return NONE\"\n",
        "}\n",
        "\n",
        "def llm_content_extractor(gpu_for_model, database, content_dic, prompt_dic):\n",
        "    for pdf_name, parts in content_dic.items():\n",
        "        for key, prompt in prompt_dic.items():\n",
        "            if key == \"abstract\":\n",
        "                abstract_entry = database.loc[database[\"file_id\"] == pdf_name, key]\n",
        "                if not abstract_entry.empty and abstract_entry.iloc[0] != \"\":\n",
        "                    continue\n",
        "\n",
        "            full_prompt = \"\"\n",
        "            if key == \"abstract\":\n",
        "                full_prompt = f\"{prompt_dic[key]} [TEXT]{parts[1] if parts[1] else parts[0]}[TEXT]\"\n",
        "            elif key == \"keywords\":\n",
        "                full_prompt = f\"{prompt_dic[key]} [TEXT]{parts[2] if parts[2] else (parts[1] if parts[1] else parts[0])}[TEXT]\"\n",
        "            else:\n",
        "                full_prompt = f\"{prompt_dic[key]} [TEXT]{parts[0] if parts[0] else parts[1]}[TEXT]\"\n",
        "\n",
        "            prompt_template = f'''SYSTEM: You are a Bibliography assistant. You give ONLY the answer as found. You NEVER add any extra words.\n",
        "                                  USER: {full_prompt}\n",
        "                                  ASSISTANT:\n",
        "            '''\n",
        "            try:\n",
        "                response = gpu_for_model(\n",
        "                    prompt=prompt_template,\n",
        "                    max_tokens=256,\n",
        "                    temperature=0.5,\n",
        "                    top_p=0.95,\n",
        "                    repeat_penalty=1.2,\n",
        "                    top_k=50,\n",
        "                    stop=['USER:', 'SYSTEM:', 'ASSISTANT:'],\n",
        "                    echo=True\n",
        "                )\n",
        "\n",
        "                assistant_response = response[\"choices\"][0][\"text\"].strip()\n",
        "                assistant_index = assistant_response.find(\"ASSISTANT:\")\n",
        "                if assistant_index != -1:\n",
        "                    assistant_response = assistant_response[assistant_index + len(\"ASSISTANT:\"):].strip()\n",
        "\n",
        "                database.loc[database[\"file_id\"] == pdf_name, key] = assistant_response\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {key} for {pdf_name}: {e}\")\n",
        "                database.loc[database[\"file_id\"] == pdf_name, key] = \"Error\"\n",
        "\n",
        "    return database\n",
        "\n",
        "\n",
        "def main_llmExtractor(database, content_dic):\n",
        "    model_name_or_path, model_basename = load_model()\n",
        "    try:\n",
        "        model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "        print(f\"Model downloaded to: {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download model: {e}\")\n",
        "        return database\n",
        "\n",
        "    try:\n",
        "        gpu_for_model = model_gpu(model_path=model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model to GPU: {e}\")\n",
        "        return database\n",
        "\n",
        "    print(\"module 4 main_llmExtractor\")\n",
        "\n",
        "    # debug the content_dic and database before processing\n",
        "    print(\"Initial database:\")\n",
        "    print(database.head())\n",
        "    print(\"Initial content_dic:\")\n",
        "    print(content_dic)\n",
        "\n",
        "    # process data\n",
        "    database = llm_content_extractor(gpu_for_model, database, content_dic, prompt_dic)\n",
        "\n",
        "    # debug the processed database\n",
        "    print(\"Processed database:\")\n",
        "    print(database.head())\n",
        "\n",
        "    return database\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8ffqu6UCH5j",
        "outputId": "2a2df1d1-540c-4cf2-fbd1-1f3dbc444663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing module4.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJ4ub7CmCH29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3EWEHveCH0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qgxe77oPV_UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. app.py"
      ],
      "metadata": {
        "id": "9z7k0nnI8qmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "from load_hf import load_huggingface_accesstoken\n",
        "from module1 import load_data\n",
        "from module2 import preprocess_data\n",
        "from module3 import create_csv_file, fillin_database\n",
        "from module4 import main_llmExtractor\n",
        "\n",
        "\n",
        "st.set_page_config(page_title='Citeasyon', page_icon=\":tada:\", layout=\"wide\")\n",
        "# HEADER\n",
        "with st.container():\n",
        "    st.title(\":red[_CITEASYON step 1_] :sparkles:\")\n",
        "    #st.subheader(\"\"\":grey[Welcome to _your PDF to text converter_ for FREE]\"\"\", divider=\"orange\")\n",
        "\n",
        "\n",
        "\n",
        "def run_citeasyon():\n",
        "  ## 0. Set up hugging face access token\n",
        "  token = st.text_input(\"Enter your Hugging Face access Token\", type= \"password\")\n",
        "  if token:\n",
        "    load_huggingface_accesstoken(token)\n",
        "  else:\n",
        "    st.warning(\"Access token for Hugging Face required\")\n",
        "  #load_huggingface_accesstoken()\n",
        "  #st.write(\"0. Set up hf\")\n",
        "\n",
        "  ## 1. Load data\n",
        "  #st.write(\"1. Load data\")\n",
        "  content_dic = load_data()\n",
        "\n",
        "\n",
        "  ## 2. Get 1000 first tokens, and get parts (up to asbtract, from abstract onwards)\n",
        "  #st.write(\"2. Get 1000 first tokens\")\n",
        "  content_dic_parts = preprocess_data(content_dic= content_dic)\n",
        "  #print(\"Content preprocessed -- OCR -- TXT --- \")\n",
        "\n",
        "  # start debugging module 2\n",
        "  #st.write(content_dic_parts.keys(), content_dic_parts.values())\n",
        "  #st.write(\"module 2: getting 3 parts:\")\n",
        "  #parts_list = [(key, value[0], value[1], value[2]) for key, value in content_dic_parts.items()]\n",
        "  #st.write(parts_list)\n",
        "  # end debugging module 2\n",
        "\n",
        "\n",
        "  # 3. Create .csv file to store information & fill in with file names, and columns: title, authors, journal, year, month, abstract, DOI, URL,...\n",
        "  database = create_csv_file()\n",
        "  #st.write(\"Database created\")\n",
        "\n",
        "  data_csv, content_dic_parts = fillin_database(database= database, content_dic = content_dic_parts)\n",
        "  st.write(\"Files to process:\", data_csv)\n",
        "\n",
        "  # 4. LLM extract information & paste into .csv file\n",
        "  structured_content = main_llmExtractor(database=data_csv, content_dic=content_dic_parts)\n",
        "  st.write(\"Structured_content\", structured_content)\n",
        "\n",
        "  st.download_button(\"Download citeasyon databse\", data=structured_content.to_csv(), file_name= \"citeasyon_database.csv\")\n",
        "  st.write(\"Now, take your citeasyon_database.csv and go to the NEXT STEP (Citeasyon step 2) to get your citatiosn into MLA or APA format\")\n",
        "\n",
        "  return structured_content\n",
        "\n",
        "\n",
        "run_citeasyon()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbN0P-GcnETe",
        "outputId": "96bb2d83-e701-44a2-bc2e-d941abc7a082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQokxwZLL1KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Run local tunnel"
      ],
      "metadata": {
        "id": "Eh72jbvlH-M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501\n"
      ],
      "metadata": {
        "id": "vBbLGnB1vzTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Mj0dpM7SF8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}